<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Tietoprovinssi Voice-Image-Edit</title>
    <link href="https://fonts.googleapis.com/css2?family=Exo:wght@400;500;600;700&display=swap" rel="stylesheet" />
    <!-- frontend backend communication -->
    <script src="https://cdn.socket.io/4.1.2/socket.io.min.js"></script>
    <!-- ort is required by vad -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.js"></script>
    <!-- vad for audio recording -->
    <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad/dist/index.browser.js"></script>
    <style>
      * {
        margin: 0;
        padding: 0;
      }
      body {
        font-family: sans-serif;
        font-family: "Exo", sans-serif;
        background: #070815;
        color: white;
        display: flex;
        flex-direction: column;
        height: 100vh;
      }

      h1 {
        text-align: center;
        font-weight: lighter;
      }

      h2 {
        text-align: center;
        font-weight: lighter;
      }

      h3 {
        text-align: center;
        font-weight: lighter;
      }

      h5 {
        text-align: center;
        font-weight: lighter;
        margin-bottom: -10px;
      }

      .p-text {
        font-size: 1.2rem;
        color: white;
        font-family: "Exo", sans-serif;
      }

      .flex-center {
        display: flex;
        align-items: center;
        justify-content: center;
      }

      .flex-horizontal {
        display: flex;
        align-items: center;
      }

      .container {
        display: flex;
        flex-direction: column;
        align-items: center;
        width: 100%;
      }

      .custom-textarea {
        flex: 1;
        margin: 0 10px;
        padding: 10px;
        color: white;
        background-color: transparent;
        border: 1px solid rgba(255, 255, 255, 0.623);
        border-radius: 5px;
        resize: none;
        font-size: 1em;
        pointer-events: none;
        font-family: "Exo", sans-serif;
        width: 300px;
        height: 100px;
      }

      .arrow {
        color: white;
        font-size: 1.5em;
        white-space: nowrap;
      }

      .sides {
        height: 100vh;
        display: flex;
      }

      .left-side {
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        padding: 20px;
        width: 30%;
      }

      .right-side {
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        flex-grow: 1;
      }

      #generated_image {
        max-width: 70%;
        max-height: 70%;
        width: auto;
        height: 100%;
        object-fit: contain;
      }
    </style>
  </head>
  <body>
    <h1>Tietoprovinssi Demo - Voice Guided AI Image Editing - GPT Lab Seinäjoki</h1>
    <div class="sides">
      <div class="left-side">
        <div class="flex-center">
          <p>Status:&nbsp;</p>
          <p id="general_status">Waiting...</p>
        </div>
        <div class="flex-center">
          <input type="radio" id="rb_auto" name="rb_lang" checked="true" />
          <label for="rb_auto">Auto</label>
          <input type="radio" id="rb_en" name="rb_lang" />
          <label for="rb_en">English</label>
          <input type="radio" id="rb_fi" name="rb_lang" />
          <label for="rb_fi">Finnish</label>
        </div>
        <div class="flex-center">
          <label> <input type="checkbox" id="toggleRecording" /> Toggle recording:&nbsp;</label>
          <p style="color: lightcoral" id="recording_status">Disabled</p>
        </div>
        <div class="container">
          <div>
            <h5>Whisper</h5>
            <h3>Transcription</h3>
            <textarea id="transcription" class="custom-textarea" tabindex="-1"></textarea>
          </div>
          <div>
            <div class="arrow">→</div>
          </div>
          <div>
            <h5>Whisper</h5>
            <h3>Translation</h3>
            <textarea id="translation" class="custom-textarea" tabindex="-1"></textarea>
          </div>
          <div>
            <div class="arrow">→</div>
          </div>
          <div>
            <h5>Mistral</h5>
            <h3>Processed</h3>
            <textarea id="llm_response" class="custom-textarea" tabindex="-1"></textarea>
          </div>
        </div>
      </div>
      <div class="right-side">
        <img id="generated_image" src="https://image.pngaaa.com/495/4827495-small.png" />
      </div>
    </div>

    <script>
      const transcription = document.getElementById("transcription");
      const translation = document.getElementById("translation");
      const llm_response = document.getElementById("llm_response");
      const general_status = document.getElementById("general_status");
      const recording_status = document.getElementById("recording_status");
      const toggleRecording = document.getElementById("toggleRecording");
      const generated_image = document.getElementById("generated_image");

      const rb_auto = document.getElementById("rb_auto");
      const rb_en = document.getElementById("rb_en");
      const rb_fi = document.getElementById("rb_fi");

      let recordingEnabled = false;
      let backendProcessing = false;
      let shouldClearTranscription = false;
      let speechTimer;
      let lastSpeechEndTime = 0;
      let myvad = null;
      let socket = null;

      toggleRecording.addEventListener("change", () => {
        recordingEnabled = toggleRecording.checked;
        updateRecordingStatusStyle();
      });

      async function startMicVAD(stream) {
        myvad = await vad.MicVAD.new({
          stream,
          positiveSpeechThreshold: 0.8,
          negativeSpeechThreshold: 0.8 - 0.15,
          minSpeechFrames: 1,
          preSpeechPadFrames: 1,
          redemptionFrames: 2,
          onSpeechStart: () => {
            if (!recordingEnabled || backendProcessing) {
              return;
            }
            if (shouldClearTranscription) {
              transcription.innerText = "";
              shouldClearTranscription = false;
            }
            console.log("Speech start");
            general_status.innerText = "Listening...";
            clearTimeout(speechTimer);
          },
          onSpeechEnd: (audio) => {
            if (!recordingEnabled || backendProcessing) {
              return;
            }
            console.log("Speech end");
            const wavBuffer = vad.utils.encodeWAV(audio);
            const base64 = vad.utils.arrayBufferToBase64(wavBuffer);
            general_status.innerText = "Processing...";
            if (socket && socket.connected) {
              socket.emit("audio_data", base64);
            }
            lastSpeechEndTime = Date.now();
            speechTimer = setTimeout(() => {
              if (Date.now() - lastSpeechEndTime >= 3000) {
                general_status.innerText = "Translating...";
                if (socket && socket.connected) {
                  socket.emit("translate");
                }
                backendProcessing = true;
                updateRecordingStatusStyle();
              }
            }, 3000);
          },
        });
        myvad.start();
      }

      async function main() {
        updateRecordingStatusStyle();
        socket = io("http://localhost:5001");

        socket.on("connect", () => {
          console.log("Connected to server");
        });

        socket.on("transcription", (data) => {
          console.log(transcription.innerText);
          general_status.innerText = "Transcription received";
          transcription.innerText = (transcription.innerText + data).replace(".", " ").replace("  ", " ");
        });

        socket.on("translation", async (data) => {
          general_status.innerText = "Translation received";
          translation.innerText = data;
          await processCommand(data);
        });

        socket.on("llm_response", async (data) => {
          general_status.innerText = "LLM response received";
          llm_response.innerText = data;
        });

        socket.on("status", (data) => {
          console.log("Status: " + data);
          general_status.innerText = data;
        });

        socket.on("image_progress", (data) => {
          console.log("Image progress received: " + data);
          generated_image.src = "data:image/png;base64," + data;
        });

        socket.on("disconnect", () => {
          console.log("Disconnected from server");
        });

        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: false,
            autoGainControl: false,
            noiseCancellation: false,
          },
          video: false,
        });
        await startMicVAD(stream);

        rb_auto.addEventListener("change", () => {
          if (socket && socket.connected) {
            socket.emit("lang_select", "");
          }
        });

        rb_en.addEventListener("change", () => {
          if (socket && socket.connected) {
            socket.emit("lang_select", "en");
          }
        });

        rb_fi.addEventListener("change", () => {
          if (socket && socket.connected) {
            socket.emit("lang_select", "fi");
          }
        });
      }
      main();

      async function processCommand(command) {
        backendProcessing = true;
        updateRecordingStatusStyle();
        general_status.innerText = "Processing command...";
        const response = await fetch("/process_command", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify({ command }),
        });
        if (response.headers.get("content-type") === "application/json") {
          general_status.innerText = "Error: " + (await response.json()).error;
          backendProcessing = false;
          shouldClearTranscription = true;
          updateRecordingStatusStyle();
          return;
        }
        const blob = await response.blob();
        const url = URL.createObjectURL(blob);
        generated_image.src = url;
        generated_image.style.display = "block";
        backendProcessing = false;
        shouldClearTranscription = true;
        updateRecordingStatusStyle();
        general_status.innerText = "Done! Waiting for input...";
      }

      function updateRecordingStatusStyle() {
        let status = "Unknown";
        recording_status.style = "";
        if (backendProcessing) {
          status = "Processing";
        } else if (recordingEnabled) {
          status = "Enabled";
        } else {
          status = "Disabled";
        }
        recording_status.innerText = status;
        if (status === "Enabled") {
          recording_status.style.color = "lightgreen";
        } else if (status === "Disabled") {
          recording_status.style.color = "lightcoral";
        } else {
          recording_status.style.color = "yellow";
        }
      }
    </script>
  </body>
</html>
